{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IST597_MLP_Regular.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnkurMali/IST597_Spring_2022/blob/main/IST597_MLP_collab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71kdFp0QgF4K"
      },
      "source": [
        "# IST597:- Multi-Layer Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2yHcl5xgPV1"
      },
      "source": [
        "## Load the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DPwxLR2gSLC"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "import keras\n",
        "import time\n",
        "import math\n",
        "import tensorflow as tf\n",
        "import torch.nn as nn\n",
        "from keras.datasets import fashion_mnist\n",
        "from numpy import sqrt\n",
        "\n",
        "seed=97238684\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "# np.random.seed(1234)\n",
        "# tf.random.set_seed(1234)\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wV-3kEaggcO8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb272b37-2eed-468e-e2b9-9f04085db541"
      },
      "source": [
        "tf.config.list_physical_devices('GPU')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the input, output and hidden layer sizes\n"
      ],
      "metadata": {
        "id": "xb2WcNmV5-pR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40XlFnwho7D8"
      },
      "source": [
        "size_input = 784\n",
        "size_hidden1 = 512\n",
        "size_hidden2 = 256\n",
        "size_hidden3 = 128\n",
        "size_output = 10\n",
        "starter_learning_rate = 0.001\n",
        "regularizer_rate = 0.1\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load the MNIST dataset and define train, test and validation sets"
      ],
      "metadata": {
        "id": "KQ7xdM7L6TbB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ratio = 0.80\n",
        "validation_ratio = 0.10\n",
        "test_ratio = 0.10\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "X = np.concatenate([X_train, X_test])\n",
        "y = np.concatenate([y_train, y_test])\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=(1-train_ratio))\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=((test_ratio/(validation_ratio+test_ratio))))\n",
        "\n",
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTQoYKheokJe",
        "outputId": "d88ae6a7-2113-4feb-a161-6f55549a7757"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(56000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load the FMNIST dataset and define train, test and validation sets"
      ],
      "metadata": {
        "id": "XkjF_wMO7wvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "(Xtrain_fmnist, ytrain_fmnist), (Xtest_fmnist, ytest_fmnist) = fashion_mnist.load_data()\n",
        "X_fmnist = np.concatenate([Xtrain_fmnist, Xtest_fmnist])\n",
        "y_fmnist = np.concatenate([ytrain_fmnist, ytest_fmnist])\n",
        "\n",
        "Xtrain_fmnist, X_val_fmnist, ytrain_fmnist, y_val_fmnist = train_test_split(X_fmnist, y_fmnist, test_size=(1-train_ratio))\n",
        "X_val_fmnist, Xtest_fmnist, y_val_fmnist, ytest_fmnist = train_test_split(X_val_fmnist, y_val_fmnist, test_size=((test_ratio/(validation_ratio+test_ratio))))"
      ],
      "metadata": {
        "id": "LbYYMC4oKS4Y"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Reshape the train, test and validation sets (MNIST)"
      ],
      "metadata": {
        "id": "GYCjFzjT6ZCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "X_train = np.reshape(X_train, (56000, 784))\n",
        "X_test = np.reshape(X_test, (-1, 784))\n",
        "X_train = X_train.astype(float) / 255\n",
        "X_test = X_test.astype(float) / 255\n",
        "X_val = np.reshape(X_val, (7000,784))\n",
        "X_val=X_val/255.0\n",
        "X_val=tf.cast(X_val,dtype=tf.float32)\n",
        "y_train = tf.one_hot(y_train,10)\n",
        "y_test = tf.one_hot(y_test,10)"
      ],
      "metadata": {
        "id": "QTSjERuunupa"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Reshape the train, test and validation sets (FMNIST)"
      ],
      "metadata": {
        "id": "5mOBLp8L75QX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Xtrain_fmnist = np.reshape(X_train, (56000, 784))\n",
        "Xtest_fmnist = np.reshape(X_test, (-1, 784))\n",
        "Xtrain_fmnist = Xtrain_fmnist.astype(float) / 255\n",
        "Xtest_fmnist = Xtest_fmnist.astype(float) / 255\n",
        "X_val_fmnist = np.reshape(X_val_fmnist, (7000,784))\n",
        "X_val_fmnist=X_val_fmnist.astype(float) /255.0\n",
        "ytrain_fmnist = tf.one_hot(ytrain_fmnist,10)\n",
        "ytest_fmnist = tf.one_hot(ytest_fmnist,10)"
      ],
      "metadata": {
        "id": "RkHsZLoAKyPc"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qm23CzRihaW0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52a4367b-e8ac-4396-be8c-7ac711cc4aff"
      },
      "source": [
        "print(X_train.shape, y_train.shape)\n",
        "print(Xtrain_fmnist.shape, ytrain_fmnist.shape)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(56000, 784) (56000, 10)\n",
            "(56000, 784) (56000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"y_train: \",y_train)\n",
        "print(\"y_val: \",y_val)\n",
        "print(\"y_test: \",y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhDAOL_VhahJ",
        "outputId": "16609efc-fe87-42f3-bd55-699631913d90"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_train:  tf.Tensor(\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 0. 1. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 1. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 1.]], shape=(56000, 10), dtype=float32)\n",
            "y_val:  [4 2 6 ... 6 2 5]\n",
            "y_test:  tf.Tensor(\n",
            "[[0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 1. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 1.]], shape=(7000, 10), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aigqKFFF5BM2"
      },
      "source": [
        "# Split MNIST dataset into batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(16)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(2)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split FMNIST dataset into batches\n",
        "train_ds_fmnist = tf.data.Dataset.from_tensor_slices((Xtrain_fmnist, ytrain_fmnist)).batch(16)\n",
        "test_ds_fmnist = tf.data.Dataset.from_tensor_slices((Xtest_fmnist, ytest_fmnist)).batch(4)\n",
        "val_ds_fmnist = tf.data.Dataset.from_tensor_slices((X_val_fmnist, y_val_fmnist)).batch(2)"
      ],
      "metadata": {
        "id": "sta64VcTK-5n"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eb4hOoVbnzSJ"
      },
      "source": [
        "## Build MLP using Eager Execution without Regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ht9_qpYipgHw"
      },
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_hidden3, self.size_output, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device \n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1]))\n",
        "    # Initialize weights between hidden layers\n",
        "    self.W_h1 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2]))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden1]))\n",
        "     # Initialize weights between hidden layers\n",
        "    self.W_h2 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3]))\n",
        "    # Initialize weights between hidden layer and output layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden2]))\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_hidden3]))\n",
        "    self.b4 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W_h1, self.W_h2, self.W2, self.b1, self.b2, self.b3, self.b4]\n",
        "\n",
        "\n",
        "        \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        "\n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    # loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred,labels=y_true)) \\\n",
        "    #     + regularizer_rate*(tf.reduce_sum(tf.square(self.b1)) + tf.reduce_sum(tf.square(self.b2)) + \\\n",
        "    #                         tf.reduce_sum(tf.square(self.b3)) + tf.reduce_sum(tf.square(self.b4)) + \\\n",
        "    #                         tf.nn.l2_loss(self.W1) + tf.nn.l2_loss(self.W_h1) + tf.nn.l2_loss(self.W_h2) + tf.nn.l2_loss(self.W2) )\n",
        "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred,labels=y_true))\n",
        "    \n",
        "    # return loss\n",
        "    \n",
        "\n",
        "\n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "    # optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "    # optimizer = tf.keras.optimizers.Adam(\n",
        "    # learning_rate=0.001, epsilon=1e-07, amsgrad=False,\n",
        "    # name='Adam')\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        "\n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    \n",
        "    # Compute values in hidden layer\n",
        "    what_i = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat_i = tf.nn.relu(what_i)\n",
        "    what_h1 = tf.matmul(hhat_i, self.W_h1) + self.b2\n",
        "    hhat_h1 = tf.nn.relu(what_h1)\n",
        "    what_h2 = tf.matmul(hhat_h1, self.W_h2) + self.b3\n",
        "    hhat_h2 = tf.nn.relu(what_h2)\n",
        "    what_out = tf.matmul(hhat_h2, self.W2) + self.b4\n",
        "    hhat = tf.nn.relu(what_out)\n",
        "    # Compute output\n",
        "    output_i = tf.matmul(hhat_i, self.W_h1) + self.b2\n",
        "    output_h1 = tf.matmul(hhat_h1, self.W_h2) + self.b3\n",
        "    output = tf.matmul(hhat_h2, self.W2) + self.b4\n",
        "    return output\n",
        "\n",
        " # Calculate standard error\n",
        "  def stderr(self,y_pred):\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    std_dev = np.std(y_pred_tf) #Calculates standard deviation\n",
        "    std_err = std_dev/sqrt(len(y_pred_tf))\n",
        "    return std_err \n",
        "\n",
        " # Calculate variance error\n",
        "  def var(self,y_pred):\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    std_dev = np.std(y_pred_tf) #Calculates standard deviation\n",
        "    variance = (std_dev**2) # calculate variance\n",
        "    return variance \n",
        "\n",
        "  def cat_accuracy(self, y_pred,y_true):\n",
        "    return tf.cast(tf.equal(tf.argmax(y_true, axis=-1),tf.argmax(y_pred, axis=-1)),tf.keras.backend.floatx())\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUDFOuNk618X"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZPVUu0YDa-_"
      },
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 10"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moAeRMJ56kr6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3c58d71-e131-466a-b284-41fb97331605"
      },
      "source": [
        "# Initialize model using CPU MNIST Data\n",
        "\n",
        "mlp_on_cpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='cpu')\n",
        "print(\"model using CPU MNIST Data Without Regularization\")\n",
        "time_start = time.time()\n",
        "acc_mnist=[]\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  accuracy=0\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(seed)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_cpu.forward(inputs)\n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "    accuracy=mlp_on_cpu.cat_accuracy(preds, outputs)\n",
        "  acc_mnist.append(np.sum(loss_total) / X_train.shape[0])\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {}, Accuracy:= {}'.format(epoch + 1, np.sum(loss_total) / X_train.shape[0], (100-(np.abs(np.sum(np.abs(accuracy)))/accuracy.shape[0]))))\n",
        "time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Average Cross Entropy:= 28.292482142857143, Accuracy:= 99.3\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 6.383518973214286, Accuracy:= 99.05\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 3.5413976004464285, Accuracy:= 99.2\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 2.3005157645089285, Accuracy:= 99.15\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 1.6026083984375, Accuracy:= 99.2\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 1.1571459263392858, Accuracy:= 99.0\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.8742852957589285, Accuracy:= 99.15\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.6637992466517857, Accuracy:= 99.15\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.5200914481026786, Accuracy:= 99.25\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.41061014229910714, Accuracy:= 99.25\n",
            "\n",
            "Total time taken (in seconds): 859.47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdMFAuH18Ve0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "110a4130-b8c4-4408-ce6b-64631cccc8ec"
      },
      "source": [
        "# Initialize model using GPU MNIST Data\n",
        "mlp_on_gpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='gpu')\n",
        "acc_mnist_gpu=[]\n",
        "print(\"model using GPU MNIST Data Without Regularization\")\n",
        "time_start = time.time()\n",
        "\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  accuracy_mnist_gpu=0\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(seed)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs)\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "    accuracy_mnist_gpu=mlp_on_cpu.cat_accuracy(preds, outputs)\n",
        "  accuracy_mnist_gpu.append(np.sum(loss_total) / X_train.shape[0])\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {}, Accuracy:= {}'.format(epoch + 1, np.sum(loss_total) / X_train.shape[0], (100-(np.abs(np.sum(np.abs(accuracy_mnist_gpu)))/accuracy_mnist_gpu.shape[0]))))\n",
        "time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model using GPU MNIST Data Without Regularization\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-932b29f22520>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mmlp_on_gpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0maccuracy_mnist_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmlp_on_cpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m   \u001b[0maccuracy_mnist_gpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_total\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Number of Epoch = {} - Average Cross Entropy:= {}, Accuracy:= {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_total\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_mnist_gpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0maccuracy_mnist_gpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         np_config.enable_numpy_behavior()\"\"\".format(type(self).__name__, name))\n\u001b[0;32m--> 442\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'append'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xI4lsqPhB6Xi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd4eb535-66be-4369-8744-5bd99f9d4112"
      },
      "source": [
        "#Default mode MNIST Data Default\n",
        "\n",
        "mlp_on_default = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output)\n",
        "acc_mnist_default=[]\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  accuracy_mnist_default=0\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(1234)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_default.forward(inputs)\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_default.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_default.loss(preds, outputs)\n",
        "    mlp_on_default.backward(inputs, outputs)\n",
        "    accuracy_mnist_default=mlp_on_cpu.cat_accuracy(preds, outputs)\n",
        "  acc_mnist_default.append(np.sum(loss_total) / X_train.shape[0])\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {}, Accuracy:= {}'.format(epoch + 1, np.sum(loss_total) / X_train.shape[0], (100-(np.abs(np.sum(np.abs(accuracy_mnist_default)))/accuracy_mnist_default.shape[0]))))\n",
        "time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Average Cross Entropy:= 0.41061014229910714, Accuracy:= 99.1\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.41061014229910714, Accuracy:= 99.05\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.41061014229910714, Accuracy:= 99.05\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.41061014229910714, Accuracy:= 99.15\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.41061014229910714, Accuracy:= 99.05\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.41061014229910714, Accuracy:= 99.0\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.41061014229910714, Accuracy:= 99.15\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.41061014229910714, Accuracy:= 99.05\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.41061014229910714, Accuracy:= 99.05\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.41061014229910714, Accuracy:= 99.05\n",
            "\n",
            "Total time taken (in seconds): 785.18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkaUg-TY7GdV",
        "outputId": "b4c9dcc1-89f4-4a9f-eb62-6c17182c21e7"
      },
      "source": [
        "#TPU mode MNIST Data using TPU\n",
        "\n",
        "mlp_on_tpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='tpu')\n",
        "accuracy_train=[]\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  accuracy=0\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(1234)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_tpu.forward(inputs)\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_tpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_tpu.loss(preds, outputs)\n",
        "    mlp_on_tpu.backward(inputs, outputs)\n",
        "    accuracy = mlp_on_tpu.cat_accuracy(preds, outputs)\n",
        "  accuracy_train.append(np.sum(loss_total_gpu) / X_train.shape[0])\n",
        "  print('Number of Epoch = {} - Average MSE:= {}, accuracy:={}'.format(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0], (100-(np.abs(np.sum(np.abs(accuracy)))/accuracy.shape[0]))))\n",
        "time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Average MSE:= 28.14095982142857, accuracy:=99.1\n",
            "Number of Epoch = 2 - Average MSE:= 6.849857700892857, accuracy:=99.05\n",
            "Number of Epoch = 3 - Average MSE:= 3.995806640625, accuracy:=99.05\n",
            "Number of Epoch = 4 - Average MSE:= 2.7440703125, accuracy:=99.1\n",
            "Number of Epoch = 5 - Average MSE:= 2.0736851283482145, accuracy:=99.15\n",
            "Number of Epoch = 6 - Average MSE:= 1.660590541294643, accuracy:=99.2\n",
            "Number of Epoch = 7 - Average MSE:= 1.3644517299107144, accuracy:=99.2\n",
            "Number of Epoch = 8 - Average MSE:= 1.1540703822544642, accuracy:=99.1\n",
            "Number of Epoch = 9 - Average MSE:= 0.9856628069196428, accuracy:=99.25\n",
            "Number of Epoch = 10 - Average MSE:= 0.8625032087053571, accuracy:=99.15\n",
            "\n",
            "Total time taken (in seconds): 927.52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model using CPU FMNIST Data\n",
        "print(\"model using CPU FMNIST Data\")\n",
        "mlp_on_cpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='cpu')\n",
        "\n",
        "time_start = time.time()\n",
        "acc_fmnist=[]\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_fmnist = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  accuracy =0\n",
        "  train_ds_fmnist = tf.data.Dataset.from_tensor_slices((Xtrain_fmnist, ytrain_fmnist)).shuffle(25, seed=epoch*(seed)).batch(20)\n",
        "  for inputs, outputs in train_ds_fmnist:\n",
        "    preds = mlp_on_cpu.forward(inputs)\n",
        "    loss_total_fmnist = loss_total_fmnist + mlp_on_cpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "    accuracy=mlp_on_cpu.cat_accuracy(preds, outputs)\n",
        "  acc_mnist.append(np.sum(loss_total_fmnist) / X_train.shape[0])\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {}, Accuracy:= {}'.format(epoch + 1, np.sum(loss_total) / X_train.shape[0], (100-(np.abs(np.sum(np.abs(accuracy)))/accuracy.shape[0]))))\n",
        "time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-2kgoT4N_ZT",
        "outputId": "44d75a34-6460-419e-e929-de05500a3455"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model using CPU FMNIST Data\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.41061014229910714, Accuracy:= 99.9\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.41061014229910714, Accuracy:= 99.9\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.41061014229910714, Accuracy:= 99.95\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.41061014229910714, Accuracy:= 99.75\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.41061014229910714, Accuracy:= 100.0\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.41061014229910714, Accuracy:= 99.95\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.41061014229910714, Accuracy:= 100.0\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.41061014229910714, Accuracy:= 99.95\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.41061014229910714, Accuracy:= 99.95\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.41061014229910714, Accuracy:= 99.95\n",
            "\n",
            "Total time taken (in seconds): 966.86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model using GPU FMNIST Data\n",
        "mlp_on_gpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='gpu')\n",
        "print(\"model using GPU FMNIST Data\")\n",
        "time_start = time.time()\n",
        "acc_fmnist=[]\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  accuracy=0\n",
        "  train_ds_fmnist = tf.data.Dataset.from_tensor_slices((Xtrain_fmnist, ytrain_fmnist)).shuffle(25, seed=epoch*(seed)).batch(20)\n",
        "  for inputs, outputs in train_ds_fmnist:\n",
        "    preds = mlp_on_gpu.forward(inputs)\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "    accuracy=mlp_on_cpu.cat_accuracy(preds, outputs)\n",
        "  acc_mnist.append(np.sum(loss_total) / Xtrain_fmnist.shape[0])\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {}, Accuracy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / Xtrain_fmnist.shape[0], (100-(np.abs(np.sum(np.abs(accuracy)))/accuracy.shape[0]))))\n",
        "time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0MX2aDiOAFt",
        "outputId": "06ebda41-44fb-46c1-9407-c40e45107e36"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model using GPU FMNIST Data\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 6.889026785714286, Accuracy:= 100.0\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 3.220023995535714, Accuracy:= 100.0\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 2.2844213169642855, Accuracy:= 99.85\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 1.6362384207589287, Accuracy:= 99.9\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 1.2818639787946429, Accuracy:= 100.0\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 1.0908824637276786, Accuracy:= 99.85\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.8864795619419643, Accuracy:= 99.95\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.7225018136160715, Accuracy:= 99.95\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.6324162248883929, Accuracy:= 100.0\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.566847900390625, Accuracy:= 100.0\n",
            "\n",
            "Total time taken (in seconds): 798.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TPU mode FMNIST Data\n",
        "\n",
        "mlp_on_gpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='tpu')\n",
        "accuracy_train_fmnist=[]\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  accuracy_fmnist_default=0\n",
        "  train_ds_fmnist = tf.data.Dataset.from_tensor_slices((Xtrain_fmnist, ytrain_fmnist)).shuffle(25, seed=epoch*(seed)).batch(20)\n",
        "  for inputs, outputs in train_ds_fmnist:\n",
        "    preds = mlp_on_gpu.forward(inputs)\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "    accuracy_fmnist_default = mlp_on_gpu.cat_accuracy(preds, outputs)\n",
        "  accuracy_train_fmnist.append(np.sum(loss_total_gpu) / Xtrain_fmnist.shape[0])\n",
        "  print('Number of Epoch = {} - Average MSE:= {}, accuracy:={}'.format(epoch + 1, np.sum(loss_total_gpu) / Xtrain_fmnist.shape[0], (100-(np.abs(np.sum(np.abs(accuracy_fmnist_default)))/accuracy_fmnist_default.shape[0]))))\n",
        "time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbKf1uLmN_bm",
        "outputId": "0cc8fbee-94a5-47a1-9153-531cebc92d6f"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Average MSE:= 6.395449776785714, accuracy:=99.95\n",
            "Number of Epoch = 2 - Average MSE:= 3.081041294642857, accuracy:=99.95\n",
            "Number of Epoch = 3 - Average MSE:= 2.0303858816964286, accuracy:=100.0\n",
            "Number of Epoch = 4 - Average MSE:= 1.5309917689732142, accuracy:=99.95\n",
            "Number of Epoch = 5 - Average MSE:= 1.2403536551339285, accuracy:=99.95\n",
            "Number of Epoch = 6 - Average MSE:= 1.0100467354910714, accuracy:=100.0\n",
            "Number of Epoch = 7 - Average MSE:= 0.8165939592633928, accuracy:=100.0\n",
            "Number of Epoch = 8 - Average MSE:= 0.6879523577008928, accuracy:=99.95\n",
            "Number of Epoch = 9 - Average MSE:= 0.59356298828125, accuracy:=99.95\n",
            "Number of Epoch = 10 - Average MSE:= 0.5306435198102678, accuracy:=99.95\n",
            "\n",
            "Total time taken (in seconds): 850.47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Default mode FMNIST Data Default\n",
        "\n",
        "mlp_on_default = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output)\n",
        "acc_fmnist_default=[]\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  accuracy_fmnist_default=0\n",
        "  train_ds_fmnist = tf.data.Dataset.from_tensor_slices((Xtrain_fmnist, ytrain_fmnist)).shuffle(25, seed=epoch*(seed)).batch(20)\n",
        "  for inputs, outputs in train_ds_fmnist:\n",
        "    preds = mlp_on_default.forward(inputs)\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_default.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_default.loss(preds, outputs)\n",
        "    mlp_on_default.backward(inputs, outputs)\n",
        "    accuracy_fmnist_default=mlp_on_cpu.cat_accuracy(preds, outputs)\n",
        "  acc_fmnist_default.append(np.sum(loss_total) / X_train.shape[0])\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {}, Accuracy:= {}'.format(epoch + 1, np.sum(loss_total) / Xtrain_fmnist.shape[0], (100-(np.abs(np.sum(np.abs(accuracy_fmnist_default)))/accuracy_fmnist_default.shape[0]))))\n",
        "time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3tyWcZrN_gU",
        "outputId": "568d11b6-cff4-456c-de49-8c9c379df0c1"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Average Cross Entropy:= 0.41061014229910714, Accuracy:= 99.95\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.41061014229910714, Accuracy:= 99.9\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.41061014229910714, Accuracy:= 99.85\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.41061014229910714, Accuracy:= 99.9\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.41061014229910714, Accuracy:= 99.9\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.41061014229910714, Accuracy:= 99.9\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.41061014229910714, Accuracy:= 99.85\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.41061014229910714, Accuracy:= 99.95\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.41061014229910714, Accuracy:= 100.0\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.41061014229910714, Accuracy:= 99.95\n",
            "\n",
            "Total time taken (in seconds): 736.35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXe-2MENCOjq"
      },
      "source": [
        "## One Step Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKxWn7CNDVN5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70c84569-367f-410e-8129-333e62e43a70"
      },
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "acc_preds = tf.Variable(0,dtype=tf.float32)\n",
        "#test_loss_total = 0.0\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_cpu.forward(inputs)\n",
        "  # print(preds)\n",
        "  b = mlp_on_cpu.loss(preds, outputs)\n",
        "  standard_error = mlp_on_cpu.stderr(preds) # Standard error\n",
        "  # print(b)\n",
        "  test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "# a = (test_loss_total.numpy() / X_train.shape[0])\n",
        "# print(X_train.shape[0])\n",
        "  Variance = mlp_on_cpu.var(preds) # Variance\n",
        "\n",
        "  for i in range(preds.shape[0]):\n",
        "    if tf.argmax(preds[i])==tf.argmax(outputs[i]):\n",
        "      acc_preds = acc_preds + 1.0\n",
        "acc = (acc_preds/X_test.shape[0])*100.0\n",
        "print('Inference 1st')\n",
        "print('Test MSE: {:.4f}'.format(np.sum(test_loss_total.numpy())/X_train.shape[0]))\n",
        "print('Accuracy: {:.4f}'.format(acc))\n",
        "print('Standard Error: {:.4f}'.format(standard_error))\n",
        "print('Variance: {:.4f}'.format(Variance))\n",
        "\n",
        "print('Test MSE: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_train.shape[0]))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 1st\n",
            "Test MSE: 0.3104\n",
            "Accuracy: 81.2000\n",
            "Standard Error: 394.6757\n",
            "Variance: 623075.6002\n",
            "Test MSE: 0.3104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "acc_preds = tf.Variable(0,dtype=tf.float32)\n",
        "#test_loss_total = 0.0\n",
        "for inputs, outputs in test_ds_fmnist:\n",
        "  preds = mlp_on_cpu.forward(inputs)\n",
        "  # print(preds)\n",
        "  b = mlp_on_cpu.loss(preds, outputs)\n",
        "  standard_error = mlp_on_cpu.stderr(preds) # Standard error\n",
        "  # print(b)\n",
        "  test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "# a = (test_loss_total.numpy() / X_train.shape[0])\n",
        "# print(X_train.shape[0])\n",
        "  Variance = mlp_on_cpu.var(preds) # Variance\n",
        "\n",
        "  for i in range(preds.shape[0]):\n",
        "    if tf.argmax(preds[i])==tf.argmax(outputs[i]):\n",
        "      acc_preds = acc_preds + 1.0\n",
        "acc = (acc_preds/Xtest_fmnist.shape[0])*100.0\n",
        "print('Inference 1st')\n",
        "print('Test MSE: {:.4f}'.format(np.sum(test_loss_total.numpy())/Xtest_fmnist.shape[0]))\n",
        "print('Accuracy: {:.4f}'.format(acc))\n",
        "print('Standard Error: {:.4f}'.format(standard_error))\n",
        "print('Variance: {:.4f}'.format(Variance))\n",
        "\n",
        "print('Test MSE: {:.4f}'.format(np.sum(test_loss_total.numpy()) / Xtest_fmnist.shape[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhxuUtDGbCKj",
        "outputId": "302af873-b57d-4775-e7b0-c240a4903c44"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 1st\n",
            "Test MSE: 3.0099\n",
            "Accuracy: 10.0857\n",
            "Standard Error: 4.5629\n",
            "Variance: 83.2797\n",
            "Test MSE: 3.0099\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "A21B--RsYV0u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}